{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t9fbEbIsGO7-",
        "outputId": "dec005b5-9d96-49c0-8bb9-d8848f70b1c9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch torchvision torchaudio --upgrade\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F"
      ],
      "metadata": {
        "id": "JiGJFgeLGxm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 5000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.2"
      ],
      "metadata": {
        "id": "vV56eEFlHC4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input2.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "R5b4bbDhHG8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)"
      ],
      "metadata": {
        "id": "B7GCxjqKHK2c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string"
      ],
      "metadata": {
        "id": "kcmLaO7UHR_E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]"
      ],
      "metadata": {
        "id": "gTtS2GH8HUSC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "wr27ay7cHWAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out"
      ],
      "metadata": {
        "id": "YBvLs6DTHX8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out"
      ],
      "metadata": {
        "id": "2uEF9w4yHaBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out"
      ],
      "metadata": {
        "id": "jdHtwG8HHeKi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)"
      ],
      "metadata": {
        "id": "aMoMNhBGHgoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "V0YbSpohHjc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ],
      "metadata": {
        "id": "O70OejkfHlOf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I5dV3O2zHpxK",
        "outputId": "f825d2d7-023b-4d8e-8204-b70bea25966d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10.788929 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-DnXXLtHt-3",
        "outputId": "017c9e00-b49f-4ade-d770-df85af89fdbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 4.2221, val loss 4.2306\n",
            "step 500: train loss 1.7600, val loss 1.9146\n",
            "step 1000: train loss 1.3903, val loss 1.5987\n",
            "step 1500: train loss 1.2644, val loss 1.5271\n",
            "step 2000: train loss 1.1835, val loss 1.4978\n",
            "step 2500: train loss 1.1233, val loss 1.4910\n",
            "step 3000: train loss 1.0718, val loss 1.4804\n",
            "step 3500: train loss 1.0179, val loss 1.5127\n",
            "step 4000: train loss 0.9604, val loss 1.5102\n",
            "step 4500: train loss 0.9125, val loss 1.5351\n",
            "step 4999: train loss 0.8589, val loss 1.5565\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBjesQWMHyaI",
        "outputId": "f0c56aa2-f08a-4ed4-d6a9-50ec0409c64b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Of your knaves, Sight and deliver for death!\n",
            "\n",
            "RIVERS:\n",
            "And so study, so delight Apollo was free!\n",
            "In hard of this brother did young Edward's slept?\n",
            "Then must I win formally-boding foot,\n",
            "In the fruit h doth mounteach to answer.\n",
            "Doth he that command more but this day,\n",
            "Do that blunt the send live thou utteriesterance\n",
            "Who it is to see him play'd with wonder fears?\n",
            "Nay, come, sit now thy name was thine, and that's touch\n",
            "Whilst I have in night enough to this:\n",
            "I, if go sleep as ill another name, and\n",
            "For beauty to pass, for the inlaw wish a\n",
            "lither eaties in the visita.\n",
            "\n",
            "FLORIZEL:\n",
            "Come, come, come, up:\n",
            "You shall be bury with me. We banish'd  for yourselves,\n",
            "Yet be best at vow, and is the wish'd, she\n",
            "married the nobles!\n",
            "\n",
            "PAULINA:\n",
            "O thou better us!\n",
            "\n",
            "AUTOLYCUS:\n",
            "For me no cause. Is it a kitcher that made you mad--in\n",
            "Would be no longer than you rogner with them,\n",
            "Your steep at use me.\n",
            "\n",
            "LEONTES:\n",
            "If I must be saY mortal honour, 'tis proved\n",
            "By drunklindness lack parting on\n",
            "And does that ever was allent diss plent;\n",
            "The counterfeit of my kindness and how mine,\n",
            "Think is son that King Lewis it made,\n",
            "The dishonour of me, in my soldiers,\n",
            "Too prepared that his officious breast need,\n",
            "King outwixt vantage, blusts for the worst comp,\n",
            "Since doubtain capars, and know will prove his honour.\n",
            "Therefore, let us in a stort shall\n",
            "Earguish with Godging hinds with a strenchange grave:\n",
            "That's more than thou, cease I can hold arms;\n",
            "And Bolingbroke serpen, they shall spend their women\n",
            "To Englishman. Which the meman mally women\n",
            "Like bears, which slily lacks, inving, and wanting greater\n",
            "With robbled inter'd fringeth, partiging,\n",
            "Returning from credering forth,\n",
            "Conclude men like a detestining sight!\n",
            "Ten oher woo sleeps lucky rose brother's daughter,\n",
            "Your kinded bodies together, or at a note;\n",
            "Edwardinate, thy father, make haste,\n",
            "I would tell him not, if thou'lt die.\n",
            "\n",
            "First Paulina:\n",
            "Make me be good, think'st me, and ne'er the end:\n",
            "'Have not Paulina masur wash'd the crown,\n",
            "And given his leisure and look will marry he.\n",
            "\n",
            "KING HARINGORY:\n",
            "Bringing the mount I may muddel of this;\n",
            "Since servily to an hour shall wish it.\n",
            "ha! worthy met, I were thy fame at Pompey,\n",
            "That thy hearw to my swear above,\n",
            "Cry froslit before me and welcome up:\n",
            "For this curst and nothing haunter\n",
            "But speeds with me;--a poor a wonry such as\n",
            "My kingdom to my soul--a-tongue they here now.\n",
            "And is this your friend husband, that unfoldited,\n",
            "Your son's spire not noble in this absecord\n",
            "Small to my modesty.\n",
            "\n",
            "LEONTEz:\n",
            "How! what me!\n",
            "Let be your sword shall not call you to't.\n",
            "Come him, sister? Welcome!\n",
            "If is her hope the white durkedom grow?\n",
            "\n",
            "AUFIDIUS:\n",
            "I say't from'ther best not stands to home.\n",
            "\n",
            "Shepherd news:\n",
            "You being now imprison't: we desire you in come\n",
            "her half into your love.\n",
            "\n",
            "AUFOLK:\n",
            "We have no ourage: your lord save our handsom,\n",
            "And proud more than out the air of Christian prisoner.\n",
            "\n",
            "Shepherd:\n",
            "Not to Norfold; then, sir.\n",
            "\n",
            "Shepherd:\n",
            "Now, my master who stuck how Claudio!\n",
            "\n",
            "Post:\n",
            "Ning honour, here he stan that will none\n",
            "Which have high beat you his grievous hand,\n",
            "His wit did like his leads pind till ignor\n",
            "What's a cause liese, commandJuliet,\n",
            "Wlet the sacrame to your brother counsel,\n",
            "Till now show'd my bodies should cover sin\n",
            "From those sweet throughld cribe deposerves;\n",
            "My ill-govern'd, the pleased mouth thereon\n",
            "Even in his guilty one and that learn'd power\n",
            "To give pus thee to aud.\n",
            "Brother's love and book the dread,\n",
            "And with God, I revenge!\n",
            "\n",
            "KING RICHARD III:\n",
            "Attle mad man in! noble madam?\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "Bulindmen! deal I celive; now\n",
            "Are that elder for interrit in hatred came with\n",
            "Thy head it in no secret by night.\n",
            "\n",
            "RICHARD:\n",
            "Sweet Warwick, had not a son weeds for patience\n",
            "And then brought hideth a word for me.\n",
            "Take his noble sweet songle thoughts of Him,\n",
            "So happy a threaten'd hilt lives;\n",
            "How neighbours my humble is the well-meet;\n",
            "I, this must body, is not safe the moon,\n",
            "Should colok the colour of this appean air,\n",
            "The land clean for the vile and disdain\n",
            "Thriver me, them, your son, your sultition\n",
            "Cannot choos prepost that; this such affect,\n",
            "How the caue on't. Camillo kick by my mother,\n",
            "He is offering abover and in accustom'd\n",
            "Showle for whispering, mutial day,\n",
            "You on the bastard, the tongue the to both\n",
            "Angelo wi luteas, the air gentleman is there,\n",
            "Your pradest selve for success a person's rite,\n",
            "A beauty freshes venge be to put you:\n",
            "If it fit be be little, he had as ha,\n",
            "'That she must a thread and worse me ape.\n",
            "What I was whither to this?\n",
            "\n",
            "HASTINGS:\n",
            "Good your grace; our father's lady's love to undance.\n",
            "Tell me, my crown father, in this bastard;\n",
            "My babest, bear and an eld more brans.\n",
            "So rude bear this, thou know'tthy strength;\n",
            "Down thine, though fear this world as white\n",
            "Benvoled forth thee: for our kinsman's woe\n",
            "Enforce deceity, though they though; but tend on us,\n",
            "Yet since lay them in extremity,\n",
            "The same you, if I were sure something mad;\n",
            "But I are made obey't,\n",
            "You chosek you of many shall, if you'r memedation,\n",
            "I of your motions and satis unthing,\n",
            "For beholdly to this young Worshire and your annoy\n",
            "The rising Perdita, and the heart himself\n",
            "My require joy wrought I not.\n",
            "\n",
            "POLIXENES:\n",
            "First, as your queen\n",
            "And young you in the stat of beauty, which you\n",
            "Would be your true Angelo?\n",
            "First half you in the dept the regal troop\n",
            "The time did affer state,--for, 'tis a man\n",
            "Jis Annopy staring, quoth youngest,\n",
            "That hath breed us a beauty still worse,--\n",
            "Whose sour wisdom of majesty's heat overjoin\n",
            "These words traitors, whilst merit, poor soul,\n",
            "Do ere me on his maffigs, penitent your will,\n",
            "And make this man to yourself beight by cross\n",
            "Against unkilly greater,--whore, that's it so,\n",
            "To whom came is promise-proclaim'd, puiltress'd;\n",
            "It with her lady; and 'tis a people,\n",
            "That your empery shall think, and proudly 'France:\n",
            "Why i' George untainshe'tngs, but, by myself,\n",
            "If some vastals of the sching to against your shide,\n",
            "Now to entertail it, or tell the world.\n",
            "\n",
            "ELBOW:\n",
            "Ay, plant is it.\n",
            "\n",
            "POMPEY:\n",
            "This muchild, sir.\n",
            "\n",
            "Nurse:\n",
            "Pritheer, I beseech you, without-like an ill thing where\n",
            "By his swift redees here about ere\n",
            "I beseech you; but cannot pass.\n",
            "\n",
            "POMPEY:\n",
            "A blood! night, and not yet with a Jack erpetual;\n",
            "And findingly it were to much brief in very moties.\n",
            "\n",
            "PRINS:\n",
            "This is a powerful, the slander years; and his,\n",
            "to lay him too, to run b't. The noble's Baynish, that\n",
            "set upon the worthy variaght, but stocked\n",
            "moniment, kneel, reverend: they saw fear of yout too,\n",
            "and who issue a shame, if they hold not a\n",
            "rick-hot never more; having sound to see,\n",
            "is interchanged hearing? hother by mother rivines\n",
            "branching with you? cost him this sins ere wreckly\n",
            "as he was thy deliver.\n",
            "\n",
            "SAMPSON:\n",
            "Yonder hatre now\n",
            "to time to aid by Prickinghabit, go back?\n",
            "\n",
            "GROMEO:\n",
            "I proud the noble ston, which is the\n",
            "wounds doth when God wou grown the man's--Were in laws being\n",
            "ood at the war. The king git is, pronounce; say me!\n",
            "\n",
            "BUGzY:\n",
            "I cannot tell; 'tis nor how I shout in my state\n",
            "Pastice of the tubsies. Yet, at all if\n",
            "Would not that une't laid on them. Innender matter, match\n",
            "Is bear-pricked to these gawds?\n",
            "You musters that, mutinius, who I think\n",
            "Wrong the unnawaranger law, and my disorries:\n",
            "When your first of Greece forth the Lady Volsce\n",
            "Were hatched with prevertiness and royal dumble\n",
            "And through 'twixt young proporter?\n",
            "\n",
            "POLIXENES:\n",
            "How, having my look to honour metry,\n",
            "I'll quiver him counting each other's son\n",
            "With disble and saily that he take his minded\n",
            "Which did the robbers; who one would forget,\n",
            "Therefore he is proof him. wherein do well?\n",
            "He did, Mark warrior; and friend his faults\n",
            "To make the world agah! and, then, by, as far\n",
            "I know him; but to show great forth,\n",
            "Which he doth tend his lebel, go, round all.\n",
            "Like a sad farthen have lived. Thou hast vent thou fin\n",
            "Hastring one high mistress to be roud,\n",
            "I deserve honour in me; though thou mayst\n",
            "Be, performittion, personicious, reison away be\n",
            "wonderver, with such and labour rought\n",
            "your prince; this with the leasing, this\n",
            "some imprison hate those good lean,\n",
            "and yet no might for le; much more retimes, yet\n",
            "soften grains of weeking the whole! he\n",
            "shall be a left a feeling and names--For this reasoning!--\n",
            "Say you ne'er little begone!--much fall'n be the\n",
            "snore that cue the cutor of the disluity\n",
            "to all.\n",
            "\n",
            "QUEEN MARGARET:\n",
            "O, thus thou shalt the best abhorn too much\n",
            "So as sudden dertilies, thy sweet wounds\n",
            "That took upon to him in the bittesting day,\n",
            "To seal it with Fron, he gives you forgive; a\n",
            "greatness for battle finger, you ne'er brought you:\n",
            "if he had gone, Pompey, or how bless the\n",
            "punishe that out took four honour, and his wish\n",
            "him above trodder the dariness. That's absent fit to do\n",
            "him a bettermit had he cause to shame letters\n",
            "you by you. This gracious man proves over your\n",
            "descenes; for I shall be compely good. Rome, he\n",
            "hath made a shy-servicing eye to me writer in that,\n",
            "he hath too do not repron his drop accusation.\n",
            "\n",
            "Heraldy:\n",
            "That heartful the isage o' ever the matter\n",
            "in't: every for the whis passes that our commended\n",
            "you bears.\n",
            "\n",
            "Shepherd:\n",
            "No, no, sirrah: why, no good nupt--to that I\n",
            "n any so, take gentle by star, but these\n",
            "accuse at the ail went can take this become of\n",
            "between patience by the wish of fair the furry.\n",
            "\n",
            "Shepherd:\n",
            "Am foul to this house, but that shall strive more\n",
            "proizeth by than his face. I'll find I with\n",
            "you ever wounds, that bait me to\n",
            "say you would set it follow, your duke's in a cause, sir?\n",
            "Your prison can seek not in the hazed him: upon him;\n",
            "so I would have thrown these towar obons with death\n",
            "intellights, the dukest and are all and the world:\n",
            "'Then yet, his good man;'--hath an honour,\n",
            "I herabled twould do't, shortly taunt, or the king,\n",
            "And in hands rode him in his world.\n",
            "Like a Volscian, or maid, and desire how\n",
            "\n",
            "Man:\n",
            "Now to die is Price, think he shall accuse\n",
            "With all the rest rest chumbing villains.\n",
            "And froward thing out his new prince's things sound;\n",
            "And when he drinkle and interrove\n",
            "And sake his scown with sightly seems myself high:\n",
            "Many draw to the breather of thy babs a tribe?\n",
            "And all them hence; and, withdraw farewells\n",
            "\n",
            "RE'T:\n",
            "Thou let me one sore that hie to be a suit of\n",
            "Friarich may present too. What's not\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3NbkydGaUqSp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}